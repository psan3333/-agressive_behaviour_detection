{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:42.147928600Z",
     "start_time": "2024-02-18T14:41:36.684034400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-02-27 09:50:14.412682: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-27 09:50:14.460391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf79f9d0e99815d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:42.180546600Z",
     "start_time": "2024-02-18T14:41:42.146931400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1367f206-b478-4dc7-a28a-ddcf3d6fcc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5d0534-c9c5-424f-9e26-b5c1371d4ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1, 2, 3]).to(device)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22da2078cd99eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:42.193552400Z",
     "start_time": "2024-02-18T14:41:42.178546700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO Возможно нужно подбирать гиперпаораметры \n",
    "IMG_SIZE = 255\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 100\n",
    "SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f7cb155daa7fef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:42.210058800Z",
     "start_time": "2024-02-18T14:41:42.194552800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10429c074462445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:44.642877800Z",
     "start_time": "2024-02-18T14:41:44.623876800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_path=\"\", df_path=\"train.csv\", img_size=224, SEQ_LENGTH=100, transform=None):\n",
    "        self.SEQ_LENGTH = SEQ_LENGTH\n",
    "        self.root_path = root_path\n",
    "        self.img_size = img_size\n",
    "        df = pd.read_csv(df_path)\n",
    "        self.video_paths = df['path'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.transform = transform\n",
    "        unique_labels = sorted(set(self.labels))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cap = cv2.VideoCapture(self.video_paths[idx])\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        frames = []\n",
    "        if total_frames >= self.SEQ_LENGTH:\n",
    "            frame_indices = np.linspace(0, total_frames - 1, self.SEQ_LENGTH, dtype=int)\n",
    "        else:\n",
    "            frame_indices = np.tile(np.arange(total_frames), self.SEQ_LENGTH // total_frames + 1)[:self.SEQ_LENGTH]\n",
    "\n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_tensor = self.transform(frame).unsqueeze(0).to(device)\n",
    "            frames.append(frame_tensor)\n",
    "\n",
    "        while len(frames) < self.SEQ_LENGTH:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        frames_tensor = torch.cat(frames, dim=0)\n",
    "\n",
    "        return frames_tensor, self.label_to_idx[self.labels[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521c638a823e9536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:45.087502300Z",
     "start_time": "2024-02-18T14:41:45.078504700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_path=\"train.csv\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_dataset = CustomDataset(df_path=\"test.csv\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e375e021bf9168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:45.936916900Z",
     "start_time": "2024-02-18T14:41:45.652800900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01125597335d5bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:46.039424100Z",
     "start_time": "2024-02-18T14:41:46.029422500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CNN_RNN(nn.Module):\n",
    "    def __init__(self, num_classes, rnn_hidden_size=256, rnn_num_layers=2):\n",
    "        super(CNN_RNN, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        ).to(device)\n",
    "\n",
    "        # LSTM\n",
    "        self.rnn = nn.LSTM(input_size=2, hidden_size=rnn_hidden_size, num_layers=rnn_num_layers, batch_first=True)\n",
    "        # полносвязанный слой для классификации\n",
    "        self.fc = nn.Linear(rnn_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Прогоняем resNetку \n",
    "        batch_size, seq_length, _, h, w = x.size()\n",
    "        x = x.view(batch_size * seq_length, 3, h, w)\n",
    "        features = self.resnet(x)\n",
    "        features = features.view(batch_size, seq_length, -1)\n",
    "        # Прогоняем RNN\n",
    "        rnn_out, _ = self.rnn(features)\n",
    "        # Получаем последний выход RNN и применяем полносвязный слой для классификации\n",
    "        output = self.fc(rnn_out[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ae3eee99789660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T14:41:46.903939Z",
     "start_time": "2024-02-18T14:41:46.627915500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# TODO Подбор архитектуры, функции ошибки и оптимайзера........\n",
    "model = CNN_RNN(2, rnn_hidden_size=8)\n",
    "\n",
    "best_model_path = 'REsnet_for_images_results/best_model_images.pt'\n",
    "model.resnet.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "model = nn.DataParallel(model, device_ids = [ 0, 1, 2, 3]).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.9), eps=1e-08, weight_decay=0.0001)\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf1d30085cfac4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-18T14:41:47.644086Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.0685280 accuracy: 0.600\n",
      "[1,    20] loss: 0.0743821 accuracy: 0.500\n",
      "[1,    30] loss: 0.0676843 accuracy: 0.600\n",
      "[1,    40] loss: 0.0824905 accuracy: 0.300\n",
      "[1,    50] loss: 0.0632768 accuracy: 0.700\n",
      "[1,    60] loss: 0.0714992 accuracy: 0.500\n",
      "[1,    70] loss: 0.0690323 accuracy: 0.550\n",
      "[1,    80] loss: 0.0730477 accuracy: 0.450\n",
      "[1,    90] loss: 0.0721888 accuracy: 0.450\n",
      "[1,   100] loss: 0.0673150 accuracy: 0.600\n",
      "[1,   110] loss: 0.0656864 accuracy: 0.650\n",
      "[1,   120] loss: 0.0671732 accuracy: 0.600\n",
      "[1,   130] loss: 0.0709078 accuracy: 0.500\n",
      "[1,   140] loss: 0.0717199 accuracy: 0.450\n",
      "[1,   150] loss: 0.0668111 accuracy: 0.650\n",
      "[1,   160] loss: 0.0670759 accuracy: 0.600\n",
      "[1,   170] loss: 0.0690820 accuracy: 0.550\n",
      "[1,   180] loss: 0.0658860 accuracy: 0.650\n",
      "[1,   190] loss: 0.0741268 accuracy: 0.400\n",
      "[1,   200] loss: 0.0717243 accuracy: 0.450\n",
      "[1,   210] loss: 0.0703489 accuracy: 0.500\n",
      "[1,   220] loss: 0.0702632 accuracy: 0.500\n",
      "[1,   230] loss: 0.0727344 accuracy: 0.350\n",
      "[1,   240] loss: 0.0690584 accuracy: 0.550\n",
      "[1,   250] loss: 0.0693205 accuracy: 0.500\n",
      "[1,   260] loss: 0.0691059 accuracy: 0.550\n",
      "[1,   270] loss: 0.0694398 accuracy: 0.500\n",
      "[1,   280] loss: 0.0695364 accuracy: 0.550\n",
      "[1,   290] loss: 0.0684253 accuracy: 0.600\n",
      "[1,   300] loss: 0.0682512 accuracy: 0.600\n",
      "[1,   310] loss: 0.0681372 accuracy: 0.600\n",
      "[1,   320] loss: 0.0689854 accuracy: 0.550\n",
      "[1,   330] loss: 0.0688951 accuracy: 0.550\n",
      "[1,   340] loss: 0.0668410 accuracy: 0.650\n",
      "[1,   350] loss: 0.0689102 accuracy: 0.550\n",
      "[1,   360] loss: 0.0690798 accuracy: 0.550\n",
      "[1,   370] loss: 0.0687920 accuracy: 0.550\n",
      "[1,   380] loss: 0.0677421 accuracy: 0.600\n",
      "[1,   390] loss: 0.0650969 accuracy: 0.700\n",
      "[1,   400] loss: 0.0675781 accuracy: 0.600\n",
      "[1,   410] loss: 0.0674786 accuracy: 0.600\n",
      "[1,   420] loss: 0.0706083 accuracy: 0.500\n",
      "[1,   430] loss: 0.0642583 accuracy: 0.700\n",
      "[1,   440] loss: 0.0691554 accuracy: 0.550\n",
      "[1,   450] loss: 0.0740086 accuracy: 0.400\n",
      "[1,   460] loss: 0.0705985 accuracy: 0.500\n",
      "[1,   470] loss: 0.0689861 accuracy: 0.550\n",
      "[1,   480] loss: 0.0713810 accuracy: 0.450\n",
      "[1,   490] loss: 0.0688359 accuracy: 0.550\n",
      "[1,   500] loss: 0.0666525 accuracy: 0.650\n",
      "[1,   510] loss: 0.0689553 accuracy: 0.550\n",
      "[1,   520] loss: 0.0688134 accuracy: 0.550\n",
      "[1,   530] loss: 0.0698956 accuracy: 0.500\n",
      "[1,   540] loss: 0.0709523 accuracy: 0.450\n",
      "[1,   550] loss: 0.0693456 accuracy: 0.500\n",
      "[1,   560] loss: 0.0706809 accuracy: 0.450\n",
      "[1,   570] loss: 0.0693606 accuracy: 0.500\n",
      "[1,   580] loss: 0.0672124 accuracy: 0.700\n",
      "[1,   590] loss: 0.0688817 accuracy: 0.550\n",
      "[1,   600] loss: 0.0703736 accuracy: 0.450\n",
      "[1,   610] loss: 0.0711304 accuracy: 0.400\n",
      "[1,   620] loss: 0.0705237 accuracy: 0.450\n",
      "[1,   630] loss: 0.0698453 accuracy: 0.450\n",
      "[1,   640] loss: 0.0689818 accuracy: 0.600\n",
      "[1,   650] loss: 0.0699994 accuracy: 0.400\n",
      "[1,   660] loss: 0.0689368 accuracy: 0.650\n",
      "[1,   670] loss: 0.0684976 accuracy: 0.650\n",
      "[1,   680] loss: 0.0687360 accuracy: 0.550\n",
      "[1,   690] loss: 0.0690015 accuracy: 0.550\n",
      "[1,   700] loss: 0.0707270 accuracy: 0.450\n",
      "[1,   710] loss: 0.0698331 accuracy: 0.500\n",
      "[1,   720] loss: 0.0672626 accuracy: 0.700\n",
      "[1,   730] loss: 0.0672174 accuracy: 0.650\n",
      "[1,   740] loss: 0.0714646 accuracy: 0.400\n",
      "[1,   750] loss: 0.0731926 accuracy: 0.300\n",
      "[1,   760] loss: 0.0684324 accuracy: 0.600\n",
      "[1,   770] loss: 0.0672598 accuracy: 0.700\n",
      "[1,   780] loss: 0.0679272 accuracy: 0.600\n",
      "[1,   790] loss: 0.0686627 accuracy: 0.550\n",
      "[1,   800] loss: 0.0677794 accuracy: 0.600\n",
      "[1,   810] loss: 0.0644888 accuracy: 0.750\n",
      "[1,   820] loss: 0.0661003 accuracy: 0.650\n",
      "[1,   830] loss: 0.0690455 accuracy: 0.550\n",
      "[1,   840] loss: 0.0691471 accuracy: 0.550\n",
      "[1,   850] loss: 0.0641714 accuracy: 0.700\n",
      "[1,   860] loss: 0.0724442 accuracy: 0.450\n",
      "[1,   870] loss: 0.0705906 accuracy: 0.500\n",
      "[1,   880] loss: 0.0750110 accuracy: 0.350\n",
      "[1,   890] loss: 0.0676298 accuracy: 0.600\n",
      "[1,   900] loss: 0.0709295 accuracy: 0.450\n",
      "[1,   910] loss: 0.0707148 accuracy: 0.450\n",
      "[1,   920] loss: 0.0681655 accuracy: 0.600\n",
      "[1,   930] loss: 0.0726712 accuracy: 0.300\n",
      "[1,   940] loss: 0.0708261 accuracy: 0.400\n",
      "Accuracy of the network on the test images: 0.570\n",
      "Model saved with accuracy: 0.570\n",
      "[2,    10] loss: 0.0688108 accuracy: 0.550\n",
      "[2,    20] loss: 0.0700633 accuracy: 0.400\n",
      "[2,    30] loss: 0.0689518 accuracy: 0.600\n",
      "[2,    40] loss: 0.0694687 accuracy: 0.500\n",
      "[2,    50] loss: 0.0660991 accuracy: 0.850\n",
      "[2,    60] loss: 0.0675228 accuracy: 0.650\n",
      "[2,    70] loss: 0.0693789 accuracy: 0.550\n",
      "[2,    80] loss: 0.0630064 accuracy: 0.800\n",
      "[2,    90] loss: 0.0690961 accuracy: 0.550\n",
      "[2,   100] loss: 0.0690080 accuracy: 0.550\n",
      "[2,   110] loss: 0.0671562 accuracy: 0.600\n",
      "[2,   120] loss: 0.0707211 accuracy: 0.500\n",
      "[2,   130] loss: 0.0724518 accuracy: 0.450\n",
      "[2,   140] loss: 0.0711306 accuracy: 0.500\n",
      "[2,   150] loss: 0.0711122 accuracy: 0.450\n",
      "[2,   160] loss: 0.0698936 accuracy: 0.500\n",
      "[2,   170] loss: 0.0706092 accuracy: 0.450\n",
      "[2,   180] loss: 0.0681674 accuracy: 0.600\n",
      "[2,   190] loss: 0.0697639 accuracy: 0.500\n",
      "[2,   200] loss: 0.0660748 accuracy: 0.700\n",
      "[2,   210] loss: 0.0701157 accuracy: 0.500\n",
      "[2,   220] loss: 0.0701662 accuracy: 0.500\n",
      "[2,   230] loss: 0.0702102 accuracy: 0.500\n",
      "[2,   240] loss: 0.0692890 accuracy: 0.500\n",
      "[2,   250] loss: 0.0665319 accuracy: 0.700\n",
      "[2,   260] loss: 0.0658821 accuracy: 0.700\n",
      "[2,   270] loss: 0.0721204 accuracy: 0.400\n",
      "[2,   280] loss: 0.0691647 accuracy: 0.550\n",
      "[2,   290] loss: 0.0677189 accuracy: 0.600\n",
      "[2,   300] loss: 0.0665071 accuracy: 0.650\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "open('training2.log', 'w').close()\n",
    "logging.basicConfig(filename='training2.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 10 == 9:  \n",
    "            print('[%d, %5d] loss: %.7f accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100, correct / total))\n",
    "            logging.info('[%d, %5d] loss: %.7f accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100, correct / total))\n",
    "            # TODO добавить данные в борд \n",
    "            writer.add_scalar('Training Loss', running_loss / 100, epoch * len(train_loader) + i)\n",
    "            writer.add_scalar('Training Accuracy', correct / total, epoch * len(train_loader) + i)\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = correct / total\n",
    "        logging.info('Accuracy of the network on the test images: %.3f' % (test_accuracy))\n",
    "        print('Accuracy of the network on the test images: %.3f' % (test_accuracy))\n",
    "        writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "        \n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            logging.info('Model saved with accuracy: %.3f' % (best_accuracy))\n",
    "            print('Model saved with accuracy: %.3f' % (best_accuracy))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94a88bd2a39a74",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148281d29e6a68e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
