{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfef71e-f066-4fe2-8337-9675aec8b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-02-24 17:11:11.471967: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-24 17:11:11.519608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import MyCustomModelV2 as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e099efc-be32-46b4-9110-e1c70aa1dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc6c49c-52b8-44a0-95dc-ca83de8558aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfedf7-b1db-436e-a138-970af992bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef2708f-cd23-4f63-9a84-463940ff7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_path=\"\", df_path=\"train.csv\", img_size=128, SEQ_LENGTH=100, transform=None):\n",
    "        self.SEQ_LENGTH = SEQ_LENGTH\n",
    "        self.root_path = root_path\n",
    "        self.img_size = img_size\n",
    "        df = pd.read_csv(df_path)\n",
    "        self.video_paths = df['path'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.transform = transform\n",
    "        unique_labels = sorted(set(self.labels))\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cap = cv2.VideoCapture(self.video_paths[idx])\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        frames = []\n",
    "        if total_frames >= self.SEQ_LENGTH:\n",
    "            frame_indices = np.linspace(0, total_frames - 1, self.SEQ_LENGTH, dtype=int)\n",
    "        else:\n",
    "            frame_indices = np.tile(np.arange(total_frames), self.SEQ_LENGTH // total_frames + 1)[:self.SEQ_LENGTH]\n",
    "\n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_tensor = self.transform(frame).unsqueeze(0)#.to(device)\n",
    "            frames.append(frame_tensor)\n",
    "\n",
    "        while len(frames) < self.SEQ_LENGTH:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        frames_tensor = torch.cat(frames, dim=0)\n",
    "        #frames_tensor = frames_tensor.permute(1,0,2,3)\n",
    "        return frames_tensor, self.label_to_idx[self.labels[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d16816-e55a-4045-9bed-7b0e1e3f3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df_path=\"train.csv\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "test_dataset = CustomDataset(df_path=\"test.csv\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d256cf-88d1-41db-a160-d4a610f8536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nx.GodHelpMe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54e2b23-7286-419c-8eec-899b22127710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(model, device_ids = [ 0, 1, 2, 3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5350dcd6-7dc6-4c3d-9d5a-88c05f3bec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Веса классов: {'notviolence': 1.837984496124031, 'violence': 2.193339500462535}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "data = pd.read_csv('data.csv')\n",
    "class_counts = data['label'].value_counts()\n",
    "total_samples = len(data)\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "print(\"Веса классов:\", class_weights)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(weight=torch.tensor([class_weights['violence'], 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5686aaec-14df-4808-afe0-50e4957016fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor([0.856]).to(device) # мб наоборот\n",
    "criterion = nn.BCELoss(weight=weights)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96e5df-4d5d-4910-8c27-3a976f46c838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10/238, Loss: 0.0600\n",
      "Epoch 1, Batch 20/238, Loss: 0.0597\n",
      "Epoch 1, Batch 30/238, Loss: 0.0590\n",
      "Epoch 1, Batch 40/238, Loss: 0.0592\n",
      "Epoch 1, Batch 50/238, Loss: 0.0618\n",
      "Epoch 1, Batch 60/238, Loss: 0.0605\n",
      "Epoch 1, Batch 70/238, Loss: 0.0624\n",
      "Epoch 1, Batch 80/238, Loss: 0.0599\n",
      "Epoch 1, Batch 90/238, Loss: 0.0630\n",
      "Epoch 1, Batch 100/238, Loss: 0.0666\n",
      "Epoch 1, Batch 110/238, Loss: 0.0607\n",
      "Epoch 1, Batch 120/238, Loss: 0.0604\n",
      "Epoch 1, Batch 130/238, Loss: 0.0596\n",
      "Epoch 1, Batch 140/238, Loss: 0.0589\n",
      "Epoch 1, Batch 150/238, Loss: 0.0568\n",
      "Epoch 1, Batch 160/238, Loss: 0.0631\n",
      "Epoch 1, Batch 170/238, Loss: 0.0615\n",
      "Epoch 1, Batch 180/238, Loss: 0.0634\n",
      "Epoch 1, Batch 190/238, Loss: 0.0588\n",
      "Epoch 1, Batch 200/238, Loss: 0.0612\n",
      "Epoch 1, Batch 210/238, Loss: 0.0584\n",
      "Epoch 1, Batch 220/238, Loss: 0.0607\n",
      "Epoch 1, Batch 230/238, Loss: 0.0590\n",
      "Test Loss: 0.5887, Accuracy: 56.96%\n",
      "Epoch 2, Batch 10/238, Loss: 0.0596\n",
      "Epoch 2, Batch 20/238, Loss: 0.0593\n",
      "Epoch 2, Batch 30/238, Loss: 0.0593\n",
      "Epoch 2, Batch 40/238, Loss: 0.0585\n",
      "Epoch 2, Batch 50/238, Loss: 0.0595\n",
      "Epoch 2, Batch 60/238, Loss: 0.0596\n",
      "Epoch 2, Batch 70/238, Loss: 0.0586\n",
      "Epoch 2, Batch 80/238, Loss: 0.0587\n",
      "Epoch 2, Batch 90/238, Loss: 0.0596\n",
      "Epoch 2, Batch 100/238, Loss: 0.0593\n",
      "Epoch 2, Batch 110/238, Loss: 0.0585\n",
      "Epoch 2, Batch 120/238, Loss: 0.0592\n",
      "Epoch 2, Batch 130/238, Loss: 0.0583\n",
      "Epoch 2, Batch 140/238, Loss: 0.0584\n",
      "Epoch 2, Batch 150/238, Loss: 0.0594\n",
      "Epoch 2, Batch 160/238, Loss: 0.0619\n",
      "Epoch 2, Batch 170/238, Loss: 0.0595\n",
      "Epoch 2, Batch 180/238, Loss: 0.0601\n",
      "Epoch 2, Batch 190/238, Loss: 0.0593\n",
      "Epoch 2, Batch 200/238, Loss: 0.0603\n",
      "Epoch 2, Batch 210/238, Loss: 0.0589\n",
      "Epoch 2, Batch 220/238, Loss: 0.0597\n",
      "Epoch 2, Batch 230/238, Loss: 0.0592\n",
      "Test Loss: 0.5839, Accuracy: 56.96%\n",
      "Epoch 3, Batch 10/238, Loss: 0.0602\n",
      "Epoch 3, Batch 20/238, Loss: 0.0610\n",
      "Epoch 3, Batch 30/238, Loss: 0.0624\n",
      "Epoch 3, Batch 40/238, Loss: 0.0599\n",
      "Epoch 3, Batch 50/238, Loss: 0.0602\n",
      "Epoch 3, Batch 60/238, Loss: 0.0591\n",
      "Epoch 3, Batch 70/238, Loss: 0.0598\n",
      "Epoch 3, Batch 80/238, Loss: 0.0598\n",
      "Epoch 3, Batch 90/238, Loss: 0.0591\n",
      "Epoch 3, Batch 100/238, Loss: 0.0590\n",
      "Epoch 3, Batch 110/238, Loss: 0.0601\n",
      "Epoch 3, Batch 120/238, Loss: 0.0595\n",
      "Epoch 3, Batch 130/238, Loss: 0.0577\n",
      "Epoch 3, Batch 140/238, Loss: 0.0611\n",
      "Epoch 3, Batch 150/238, Loss: 0.0604\n",
      "Epoch 3, Batch 160/238, Loss: 0.0598\n",
      "Epoch 3, Batch 170/238, Loss: 0.0593\n",
      "Epoch 3, Batch 180/238, Loss: 0.0608\n",
      "Epoch 3, Batch 190/238, Loss: 0.0599\n",
      "Epoch 3, Batch 200/238, Loss: 0.0595\n",
      "Epoch 3, Batch 210/238, Loss: 0.0593\n",
      "Epoch 3, Batch 220/238, Loss: 0.0583\n",
      "Epoch 3, Batch 230/238, Loss: 0.0600\n",
      "Test Loss: 0.5837, Accuracy: 56.96%\n",
      "Epoch 4, Batch 10/238, Loss: 0.0581\n",
      "Epoch 4, Batch 20/238, Loss: 0.0593\n",
      "Epoch 4, Batch 30/238, Loss: 0.0597\n",
      "Epoch 4, Batch 40/238, Loss: 0.0602\n",
      "Epoch 4, Batch 50/238, Loss: 0.0574\n",
      "Epoch 4, Batch 60/238, Loss: 0.0598\n",
      "Epoch 4, Batch 70/238, Loss: 0.0593\n",
      "Epoch 4, Batch 80/238, Loss: 0.0597\n",
      "Epoch 4, Batch 90/238, Loss: 0.0596\n",
      "Epoch 4, Batch 100/238, Loss: 0.0599\n",
      "Epoch 4, Batch 110/238, Loss: 0.0591\n",
      "Epoch 4, Batch 120/238, Loss: 0.0583\n",
      "Epoch 4, Batch 130/238, Loss: 0.0601\n",
      "Epoch 4, Batch 140/238, Loss: 0.0614\n",
      "Epoch 4, Batch 150/238, Loss: 0.0607\n",
      "Epoch 4, Batch 160/238, Loss: 0.0603\n",
      "Epoch 4, Batch 170/238, Loss: 0.0590\n",
      "Epoch 4, Batch 180/238, Loss: 0.0596\n",
      "Epoch 4, Batch 190/238, Loss: 0.0594\n",
      "Epoch 4, Batch 200/238, Loss: 0.0593\n",
      "Epoch 4, Batch 210/238, Loss: 0.0595\n",
      "Epoch 4, Batch 220/238, Loss: 0.0597\n",
      "Epoch 4, Batch 230/238, Loss: 0.0592\n",
      "Test Loss: 0.5875, Accuracy: 56.96%\n",
      "Epoch 5, Batch 10/238, Loss: 0.0577\n",
      "Epoch 5, Batch 20/238, Loss: 0.0613\n",
      "Epoch 5, Batch 30/238, Loss: 0.0590\n",
      "Epoch 5, Batch 40/238, Loss: 0.0603\n",
      "Epoch 5, Batch 50/238, Loss: 0.0590\n",
      "Epoch 5, Batch 60/238, Loss: 0.0579\n",
      "Epoch 5, Batch 70/238, Loss: 0.0596\n",
      "Epoch 5, Batch 80/238, Loss: 0.0588\n",
      "Epoch 5, Batch 90/238, Loss: 0.0592\n",
      "Epoch 5, Batch 100/238, Loss: 0.0590\n",
      "Epoch 5, Batch 110/238, Loss: 0.0607\n",
      "Epoch 5, Batch 120/238, Loss: 0.0589\n",
      "Epoch 5, Batch 130/238, Loss: 0.0593\n",
      "Epoch 5, Batch 140/238, Loss: 0.0593\n",
      "Epoch 5, Batch 150/238, Loss: 0.0594\n",
      "Epoch 5, Batch 160/238, Loss: 0.0593\n",
      "Epoch 5, Batch 170/238, Loss: 0.0595\n",
      "Epoch 5, Batch 180/238, Loss: 0.0589\n",
      "Epoch 5, Batch 190/238, Loss: 0.0588\n",
      "Epoch 5, Batch 200/238, Loss: 0.0576\n",
      "Epoch 5, Batch 210/238, Loss: 0.0587\n",
      "Epoch 5, Batch 220/238, Loss: 0.0590\n",
      "Epoch 5, Batch 230/238, Loss: 0.0615\n",
      "Test Loss: 0.5838, Accuracy: 56.96%\n",
      "Epoch 6, Batch 10/238, Loss: 0.0598\n",
      "Epoch 6, Batch 20/238, Loss: 0.0595\n",
      "Epoch 6, Batch 30/238, Loss: 0.0593\n",
      "Epoch 6, Batch 40/238, Loss: 0.0593\n",
      "Epoch 6, Batch 50/238, Loss: 0.0593\n",
      "Epoch 6, Batch 60/238, Loss: 0.0594\n",
      "Epoch 6, Batch 70/238, Loss: 0.0591\n",
      "Epoch 6, Batch 80/238, Loss: 0.0594\n",
      "Epoch 6, Batch 90/238, Loss: 0.0596\n",
      "Epoch 6, Batch 100/238, Loss: 0.0584\n",
      "Epoch 6, Batch 110/238, Loss: 0.0597\n",
      "Epoch 6, Batch 120/238, Loss: 0.0596\n",
      "Epoch 6, Batch 130/238, Loss: 0.0602\n",
      "Epoch 6, Batch 140/238, Loss: 0.0593\n",
      "Epoch 6, Batch 150/238, Loss: 0.0595\n",
      "Epoch 6, Batch 160/238, Loss: 0.0602\n",
      "Epoch 6, Batch 170/238, Loss: 0.0603\n",
      "Epoch 6, Batch 180/238, Loss: 0.0594\n",
      "Epoch 6, Batch 190/238, Loss: 0.0594\n",
      "Epoch 6, Batch 200/238, Loss: 0.0571\n",
      "Epoch 6, Batch 210/238, Loss: 0.0587\n",
      "Epoch 6, Batch 220/238, Loss: 0.0596\n",
      "Epoch 6, Batch 230/238, Loss: 0.0615\n",
      "Test Loss: 0.5843, Accuracy: 56.96%\n",
      "Epoch 7, Batch 10/238, Loss: 0.0578\n",
      "Epoch 7, Batch 20/238, Loss: 0.0591\n",
      "Epoch 7, Batch 30/238, Loss: 0.0595\n",
      "Epoch 7, Batch 40/238, Loss: 0.0620\n",
      "Epoch 7, Batch 50/238, Loss: 0.0597\n",
      "Epoch 7, Batch 60/238, Loss: 0.0598\n",
      "Epoch 7, Batch 70/238, Loss: 0.0591\n",
      "Epoch 7, Batch 80/238, Loss: 0.0611\n",
      "Epoch 7, Batch 90/238, Loss: 0.0595\n",
      "Epoch 7, Batch 100/238, Loss: 0.0579\n",
      "Epoch 7, Batch 110/238, Loss: 0.0594\n",
      "Epoch 7, Batch 120/238, Loss: 0.0574\n",
      "Epoch 7, Batch 130/238, Loss: 0.0614\n",
      "Epoch 7, Batch 140/238, Loss: 0.0587\n",
      "Epoch 7, Batch 150/238, Loss: 0.0594\n",
      "Epoch 7, Batch 160/238, Loss: 0.0593\n",
      "Epoch 7, Batch 170/238, Loss: 0.0593\n",
      "Epoch 7, Batch 180/238, Loss: 0.0592\n",
      "Epoch 7, Batch 190/238, Loss: 0.0589\n",
      "Epoch 7, Batch 200/238, Loss: 0.0595\n",
      "Epoch 7, Batch 210/238, Loss: 0.0608\n",
      "Epoch 7, Batch 220/238, Loss: 0.0591\n",
      "Epoch 7, Batch 230/238, Loss: 0.0595\n",
      "Test Loss: 0.5885, Accuracy: 56.96%\n",
      "Epoch 8, Batch 10/238, Loss: 0.0586\n",
      "Epoch 8, Batch 20/238, Loss: 0.0600\n",
      "Epoch 8, Batch 30/238, Loss: 0.0592\n",
      "Epoch 8, Batch 40/238, Loss: 0.0597\n",
      "Epoch 8, Batch 50/238, Loss: 0.0611\n",
      "Epoch 8, Batch 60/238, Loss: 0.0596\n",
      "Epoch 8, Batch 70/238, Loss: 0.0596\n",
      "Epoch 8, Batch 80/238, Loss: 0.0600\n",
      "Epoch 8, Batch 90/238, Loss: 0.0595\n",
      "Epoch 8, Batch 100/238, Loss: 0.0595\n",
      "Epoch 8, Batch 110/238, Loss: 0.0589\n",
      "Epoch 8, Batch 120/238, Loss: 0.0585\n",
      "Epoch 8, Batch 130/238, Loss: 0.0600\n",
      "Epoch 8, Batch 140/238, Loss: 0.0594\n",
      "Epoch 8, Batch 150/238, Loss: 0.0593\n",
      "Epoch 9, Batch 230/238, Loss: 0.0592\n",
      "Test Loss: 0.5906, Accuracy: 56.96%\n",
      "Epoch 10, Batch 10/238, Loss: 0.0593\n",
      "Epoch 10, Batch 20/238, Loss: 0.0604\n",
      "Epoch 10, Batch 30/238, Loss: 0.0598\n",
      "Epoch 10, Batch 40/238, Loss: 0.0606\n",
      "Epoch 10, Batch 50/238, Loss: 0.0593\n",
      "Epoch 10, Batch 60/238, Loss: 0.0605\n",
      "Epoch 10, Batch 70/238, Loss: 0.0602\n",
      "Epoch 10, Batch 80/238, Loss: 0.0584\n",
      "Epoch 10, Batch 90/238, Loss: 0.0604\n",
      "Epoch 10, Batch 100/238, Loss: 0.0594\n",
      "Epoch 10, Batch 110/238, Loss: 0.0585\n",
      "Epoch 10, Batch 120/238, Loss: 0.0588\n",
      "Epoch 10, Batch 130/238, Loss: 0.0585\n",
      "Epoch 10, Batch 140/238, Loss: 0.0597\n",
      "Epoch 10, Batch 150/238, Loss: 0.0600\n",
      "Epoch 10, Batch 160/238, Loss: 0.0597\n",
      "Epoch 10, Batch 170/238, Loss: 0.0592\n",
      "Epoch 10, Batch 180/238, Loss: 0.0594\n",
      "Epoch 10, Batch 190/238, Loss: 0.0596\n",
      "Epoch 10, Batch 200/238, Loss: 0.0593\n",
      "Epoch 10, Batch 210/238, Loss: 0.0589\n",
      "Epoch 10, Batch 220/238, Loss: 0.0588\n",
      "Epoch 10, Batch 230/238, Loss: 0.0595\n",
      "Test Loss: 0.5837, Accuracy: 56.96%\n",
      "Epoch 11, Batch 10/238, Loss: 0.0605\n",
      "Epoch 11, Batch 20/238, Loss: 0.0584\n",
      "Epoch 11, Batch 30/238, Loss: 0.0573\n",
      "Epoch 11, Batch 40/238, Loss: 0.0614\n",
      "Epoch 11, Batch 50/238, Loss: 0.0599\n",
      "Epoch 11, Batch 60/238, Loss: 0.0572\n",
      "Epoch 11, Batch 70/238, Loss: 0.0593\n",
      "Epoch 11, Batch 80/238, Loss: 0.0597\n",
      "Epoch 11, Batch 90/238, Loss: 0.0599\n",
      "Epoch 11, Batch 100/238, Loss: 0.0592\n",
      "Epoch 11, Batch 110/238, Loss: 0.0593\n",
      "Epoch 11, Batch 120/238, Loss: 0.0594\n",
      "Epoch 11, Batch 130/238, Loss: 0.0593\n",
      "Epoch 11, Batch 140/238, Loss: 0.0595\n",
      "Epoch 11, Batch 150/238, Loss: 0.0594\n",
      "Epoch 11, Batch 160/238, Loss: 0.0591\n",
      "Epoch 11, Batch 170/238, Loss: 0.0596\n",
      "Epoch 11, Batch 180/238, Loss: 0.0582\n",
      "Epoch 11, Batch 190/238, Loss: 0.0580\n",
      "Epoch 11, Batch 200/238, Loss: 0.0602\n",
      "Epoch 11, Batch 210/238, Loss: 0.0615\n",
      "Epoch 11, Batch 220/238, Loss: 0.0591\n",
      "Epoch 11, Batch 230/238, Loss: 0.0596\n",
      "Test Loss: 0.5884, Accuracy: 56.96%\n",
      "Epoch 12, Batch 10/238, Loss: 0.0585\n",
      "Epoch 12, Batch 20/238, Loss: 0.0591\n",
      "Epoch 12, Batch 30/238, Loss: 0.0605\n",
      "Epoch 12, Batch 40/238, Loss: 0.0585\n",
      "Epoch 12, Batch 50/238, Loss: 0.0584\n",
      "Epoch 12, Batch 60/238, Loss: 0.0591\n",
      "Epoch 12, Batch 70/238, Loss: 0.0578\n",
      "Epoch 12, Batch 80/238, Loss: 0.0583\n",
      "Epoch 12, Batch 90/238, Loss: 0.0580\n",
      "Epoch 12, Batch 100/238, Loss: 0.0637\n",
      "Epoch 12, Batch 110/238, Loss: 0.0605\n",
      "Epoch 12, Batch 120/238, Loss: 0.0586\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Конфигурация логирования\n",
    "logging.basicConfig(filename='ЗАРАБОТАЙ УМОЛЯЮ.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Переменная для хранения наивысшей точности\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Переводим модель в режим обучения\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Обнуляем градиенты параметров\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход: вычисляем предсказанные значения\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Вычисляем потери\n",
    "        loss = criterion(outputs, target.float().view(-1, 1))\n",
    "\n",
    "        # Обратное распространение и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Считаем общие потери\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Выводим промежуточные результаты в терминал\n",
    "        if batch_idx % 10 == 9:  # Выводим каждые 100 батчей\n",
    "            print(f'Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {running_loss / 100:.4f}')\n",
    "            logging.info(f'Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Оценка модели после каждой эпохи\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            test_loss += criterion(outputs, target.float().view(-1, 1)).item()\n",
    "            predicted = torch.round(outputs)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target.float().view(-1, 1)).sum().item()\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    logging.info(f'Test Loss: {test_loss / len(test_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    # Сохраняем модель, если достигнута лучшая точность\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'lastWay.pth')\n",
    "        logging.info('Best model saved.')\n",
    "    torch.save(model.state_dict(), 'LAASTTTlastWay.pth')\n",
    "\n",
    "logging.info('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a40aa5-12ca-4dd8-8d9b-56eedc708755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
